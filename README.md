# IntelDesk
Real time Speech Emotion Recognition for Archetypal Emotions <br>
Prepared by:
- Fatima Fouda
- Yosif Atef

Supervised by: <br>
- Noha Younis

## Intro
The main aim of the proposed system is designing an automatic speech emotion recognition system for the archetypal emotions.  The main objectives are combining several speech features, of low computational complexity, to achieve real time performance and improve the ASER accuracy. Also the system targets two different languages for the applied database; that are Arabic and English. The proposed system should be also user independent. 

The main algorithms of the proposed system are designed based on signal processing techniques for speech recognition. The K-Nearest Neighbor classifier (K-NN) was used and its algorithm was modified to improve the accuracy rate. The system was designed and tested using MATLAB then it was compared with the most applied current systems. Yet, the range of accuracy achieved is as high as **95.8%** for the English databases used for Happiness, Anger, Sadness, Fear and Neutral, based on the selected speech features and the modified K-NN classifier. The Arabic databaseâ€™s accuracy range is as high as **81.8%** for Happiness, Anger and Sadness.

## Flowchart
![Flowchart](https://github.com/fatimafouda/IntelDesk/blob/assets/assets/Flowchart.png)

## Datasets

- [Surrey Audio-Visual Expressed Emotion (SAVEE)](http://kahlan.eps.surrey.ac.uk/savee/Database.html)
- [Toronto emotional speech set (TESS)](https://tspace.library.utoronto.ca/handle/1807/24487)

## Results
![Results](https://github.com/fatimafouda/IntelDesk/blob/assets/assets/Paper%20Comparision.png)


